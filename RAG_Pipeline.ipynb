{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fc9c46-b08b-4a0a-ab6b-65a305f4d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone\n",
      "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pinecone) (2025.1.31)\n",
      "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
      "  Downloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pinecone) (4.14.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pinecone) (2.5.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl (259 kB)\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-assistant, pinecone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pinecone]2/3\u001b[0m [pinecone]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pinecone-7.3.0 pinecone-plugin-assistant-1.8.0 pinecone-plugin-interface-0.0.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ad7967-3e5f-4237-82ac-e4b3481c14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Ingest\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7fd0b-ac91-449a-b990-0c7ad79fda47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing clients and models ---\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP AND INITIALIZATION ---\n",
    "print(\"--- Initializing clients and models ---\")\n",
    "load_dotenv() # Load variables from .env file\n",
    "# Configure Gemini (we don't use it here, but good practice)\n",
    "#genai.configure(api_key='__')\n",
    "genai.configure(api_key='')\n",
    "# Connect to Pinecone\n",
    "#pc = Pinecone(api_key='__'\n",
    "pc = Pinecone(api_key='')             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f53fd0-1dba-4738-924f-7a37aae1dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DEFINE CONSTANTS ---\n",
    "EMBED_MODEL_NAME = 'all-MiniLM-L6-v2' # 384 dimensions\n",
    "DIMENSION = 384\n",
    "INDEX_NAME = \"coffeeindex\" # Give our index a name\n",
    "DOCS_NS = \"docs\" # The namespace for our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7bd3389-abea-4b67-9cd0-39e7de448d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our \"Knowledge Base\" of coffee documents\n",
    "DOCUMENTS = [\n",
    "    {\n",
    "        \"id\": \"doc-1\",\n",
    "        \"text\": \"Ashwagandha coffee is a beverage that blends coffee with Ashwagandha root powder, an adaptogen used in Ayurvedic medicine. It's claimed to help reduce stress and anxiety.\",\n",
    "        \"metadata\": {\"title\": \"Ashwagandha Coffee\", \"category\": \"Herbal Blends\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc-2\",\n",
    "        \"text\": \"A turmeric latte, also known as 'golden milk', is a traditional caffeine-free drink. It's made with milk (or a non-dairy alternative), turmeric, ginger, cinnamon, and a sweetener. It is prized for its anti-inflammatory properties.\",\n",
    "        \"metadata\": {\"title\": \"Turmeric Latte\", \"category\": \"Caffeine-Free\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc-3\",\n",
    "        \"text\": \"For a standard espresso shot, the typical brew ratio is 1:2, meaning 18 grams of ground coffee yields a 36-gram liquid shot in about 25-30 seconds.\",\n",
    "        \"metadata\": {\"title\": \"Espresso Brew Ratios\", \"category\": \"Brewing\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc-4\",\n",
    "        \"text\": \"Mushroom coffee, such as Chaga or Lion's Mane, is a blend that offers lower caffeine levels than regular coffee. It's often promoted for its cognitive and immune-boosting benefits.\",\n",
    "        \"metadata\": {\"title\": \"Mushroom Coffee\", \"category\": \"Herbal Blends\"}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f139cb3-11f3-48a2-8434-3fd329859843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if index 'coffeeindex' exists...\n",
      "Index not found. Creating a new serverless index: coffeeindex\n",
      "Successfully connected to index: 'coffeeindex'\n"
     ]
    }
   ],
   "source": [
    "# --- 3. CREATE INDEX (IF NOT EXISTS) ---\n",
    "print(f\"Checking if index '{INDEX_NAME}' exists...\")\n",
    "if INDEX_NAME not in [i[\"name\"] for i in pc.list_indexes()]:\n",
    "    print(f\"Index not found. Creating a new serverless index: {INDEX_NAME}\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=DIMENSION,\n",
    "        metric=\"cosine\", # Cosine similarity is great for semantic search\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    # Wait for the index to be ready\n",
    "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
    "        print(\"Waiting for index to be ready...\")\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(f\"Successfully connected to index: '{INDEX_NAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9edc7b51-1393-4dcb-b7dd-3bd21aa213d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Embedding 4 documents...\n",
      "Upserting 4 vectors into namespace 'docs'...\n",
      "Namespace 'docs' does not exist yet. Skipping delete.\n",
      "--- Ingestion Complete ---\n",
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'docs': {'vector_count': 4}},\n",
      " 'total_vector_count': 4,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# --- 4. EMBED AND UPSERT DOCUMENTS ---\n",
    "print(f\"Loading embedding model: {EMBED_MODEL_NAME}...\")\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "print(f\"Embedding {len(DOCUMENTS)} documents...\")\n",
    "texts = [doc['text'] for doc in DOCUMENTS]\n",
    "embeddings = embedder.encode(texts, normalize_embeddings=True).tolist()\n",
    "\n",
    "# Prepare vectors for upsert\n",
    "vectors_to_upsert = []\n",
    "for i, doc in enumerate(DOCUMENTS):\n",
    "    vectors_to_upsert.append({\n",
    "        \"id\": doc['id'],\n",
    "        \"values\": embeddings[i],\n",
    "        \"metadata\": {\n",
    "            \"title\": doc['metadata']['title'],\n",
    "            \"text\": doc['text']\n",
    "        }\n",
    "    })\n",
    "print(f\"Upserting {len(vectors_to_upsert)} vectors into namespace '{DOCS_NS}'...\")\n",
    "# ✅ FIX: Only delete namespace if it already exists\n",
    "try:\n",
    "    stats = index.describe_index_stats()\n",
    "    if DOCS_NS in stats.get(\"namespaces\", {}):\n",
    "        print(f\"Clearing namespace '{DOCS_NS}' before upserting...\")\n",
    "        index.delete(delete_all=True, namespace=DOCS_NS)\n",
    "    else:\n",
    "        print(f\"Namespace '{DOCS_NS}' does not exist yet. Skipping delete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not check or clear namespace '{DOCS_NS}'. Continuing anyway. ({e})\")\n",
    "\n",
    "# Now upsert your documents\n",
    "index.upsert(vectors=vectors_to_upsert, namespace=DOCS_NS)\n",
    "\n",
    "print(\"--- Ingestion Complete ---\")\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249b57e9-60a8-42db-a929-06567fb834b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients and models loaded.\n"
     ]
    }
   ],
   "source": [
    "#2.basic_rag_bot.py\n",
    "\n",
    "# Initialize models\n",
    "EMBED_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "llm = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# Connect to our Pinecone index\n",
    "INDEX_NAME = \"coffeeindex\"\n",
    "DOCS_NS = \"docs\"\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(\"Clients and models loaded.\")\n",
    "\n",
    "# --- 2. DEFINE THE PROMPT TEMPLATE ---\n",
    "# We define the template as a simple string, NO YAML needed [cite: 3484-3485]\n",
    "BASIC_RAG_TEMPLATE = \"\"\"\n",
    "You are a helpful coffee expert. Use the following CONTEXT to answer the QUESTION.\n",
    "If the answer is not in the context, say \"I don't know from the provided documents.\"\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef0beacd-2c2d-49bb-91b6-9be2c447f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. THE BASIC RAG PIPELINE FUNCTION ---\n",
    "def basic_rag(query: str, k: int = 2):\n",
    "    \"\"\"Performs a complete, stateless RAG lookup.\"\"\"\n",
    "    print(f\"\\n--- Basic RAG Query ---\")\n",
    "    print(f\"User: {query}\")\n",
    "\n",
    "    # 1. RETRIEVE\n",
    "    print(f\"Retrieving top-{k} documents...\")\n",
    "    query_vector = embedder.encode([query], normalize_embeddings=True)[0].tolist()\n",
    "    search_results = index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=k,\n",
    "        include_metadata=True, # We need the metadata to get the text\n",
    "        namespace=DOCS_NS\n",
    "    )\n",
    "\n",
    "    # 2. COMPOSE PROMPT\n",
    "    contexts = []\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for match in search_results[\"matches\"]:\n",
    "        text = match['metadata'].get(\"text\", \"No text metadata found\")\n",
    "        contexts.append(text)\n",
    "        print(f\"  - (Score: {match['score']:.4f}) {text[:80]}...\")\n",
    "\n",
    "    joined_context = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "\n",
    "    # Fill the prompt template\n",
    "    prompt = BASIC_RAG_TEMPLATE.format(query=query, context=joined_context)\n",
    "    print(\"\\n\"*5)\n",
    "    print(joined_context)\n",
    "    print(\"\\n\"*5)\n",
    "    print(prompt)\n",
    "    print(\"\\n\"*5)\n",
    "\n",
    "    # 3. GENERATE\n",
    "    print(\"Calling Gemini to generate a grounded answer...\")\n",
    "    response = llm.generate_content(prompt)\n",
    "\n",
    "    print(f\"\\nAssistant: {response.text}\")\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f17cd30-d5d3-4f12-8c57-063287a53fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Basic RAG Query ---\n",
      "User: What is Ashwagandha coffee?\n",
      "Retrieving top-2 documents...\n",
      "Retrieved Documents:\n",
      "  - (Score: 0.8663) Ashwagandha coffee is a beverage that blends coffee with Ashwagandha root powder...\n",
      "  - (Score: 0.4822) Mushroom coffee, such as Chaga or Lion's Mane, is a blend that offers lower caff...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ashwagandha coffee is a beverage that blends coffee with Ashwagandha root powder, an adaptogen used in Ayurvedic medicine. It's claimed to help reduce stress and anxiety.\n",
      "\n",
      "---\n",
      "\n",
      "Mushroom coffee, such as Chaga or Lion's Mane, is a blend that offers lower caffeine levels than regular coffee. It's often promoted for its cognitive and immune-boosting benefits.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are a helpful coffee expert. Use the following CONTEXT to answer the QUESTION.\n",
      "If the answer is not in the context, say \"I don't know from the provided documents.\"\n",
      "\n",
      "QUESTION:\n",
      "What is Ashwagandha coffee?\n",
      "\n",
      "CONTEXT:\n",
      "Ashwagandha coffee is a beverage that blends coffee with Ashwagandha root powder, an adaptogen used in Ayurvedic medicine. It's claimed to help reduce stress and anxiety.\n",
      "\n",
      "---\n",
      "\n",
      "Mushroom coffee, such as Chaga or Lion's Mane, is a blend that offers lower caffeine levels than regular coffee. It's often promoted for its cognitive and immune-boosting benefits.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Calling Gemini to generate a grounded answer...\n",
      "\n",
      "Assistant: Ashwagandha coffee is a beverage that blends coffee with Ashwagandha root powder, an adaptogen used in Ayurvedic medicine. It's claimed to help reduce stress and anxiety.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. RUN THE DEMO ---\n",
    "if __name__ == \"__main__\":\n",
    "    basic_rag(\"What is Ashwagandha coffee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a5c627a-b509-4ab5-a5b4-0fbc6684787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++ RAG with Memory +++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c63d87fe-cc4d-4d56-af09-d0ac21658dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# --- 3. HELPER FUNCTIONS (Memory, Retrieval, Formatting) ---\n",
    "\n",
    "def extract_user_facts(user_text: str) -> List[str]:\n",
    "    \"\"\"A simple regex extractor for demo purposes.\"\"\"\n",
    "    # This looks for phrases like \"I like...\", \"I prefer...\", \"I avoid...\", \"I am...\"\n",
    "    pats = [r\"\\bI (?:like|love|prefer)\\b[^.]+\", r\"\\bI (?:avoid|usually|often|am)\\b[^.]+\"]\n",
    "    findings = []\n",
    "    for pat in pats:\n",
    "        findings += [m.group(0).strip() for m in re.finditer(pat, user_text, flags=re.I)]\n",
    "    return sorted(set(findings))\n",
    "\n",
    "def add_memory_facts(facts: List[str]) -> None:\n",
    "    \"\"\"Embeds and upserts facts into the user's memory namespace.\"\"\"\n",
    "    if not facts:\n",
    "        return\n",
    "\n",
    "    print(f\"  [Memory System: Found {len(facts)} new fact(s) to remember.]\")\n",
    "    vecs = embedder.encode(facts, normalize_embeddings=True).tolist()\n",
    "    now = int(time.time())\n",
    "    payload = []\n",
    "    for i, fact in enumerate(facts):\n",
    "        vid = f\"{USER_ID}:{now}:{i}\" # Create a unique ID for the memory vector\n",
    "        meta = {\"fact\": fact, \"user_id\": USER_ID, \"ts\": now}\n",
    "        # Use tuple format (id, vector, metadata) for upsert\n",
    "        payload.append((vid, vecs[i], meta))\n",
    "\n",
    "    # Upsert into the user's dedicated MEM_NS namespace\n",
    "    index.upsert(vectors=payload, namespace=MEM_NS)\n",
    "\n",
    "def retrieve_memory(query_text: str, k: int = 2) -> List[Dict]:\n",
    "    \"\"\"Retrieves relevant facts from the user's memory namespace.\"\"\"\n",
    "    qv = embedder.encode([query_text], normalize_embeddings=True)[0].tolist()\n",
    "    res = index.query(vector=qv, top_k=k, include_metadata=True, namespace=MEM_NS)\n",
    "    return res.get(\"matches\", [])\n",
    "\n",
    "def retrieve_docs(query_text: str, k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Retrieves relevant documents from the main 'docs' namespace.\"\"\"\n",
    "    qv = embedder.encode([query_text], normalize_embeddings=True)[0].tolist()\n",
    "    res = index.query(vector=qv, top_k=k, include_metadata=True, namespace=DOCS_NS)\n",
    "    return res.get(\"matches\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e1d5d82-f5ff-4676-9c06-af895713fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Coffee Memory-RAG Bot ---\n",
      "Type 'exit' to quit. Try stating a preference, like 'I prefer low-caffeine drinks.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  what is the name of coffee i did not like \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Retrieving docs for: 'what is the name of coffee i did not like']\n",
      "  [Retrieving memories for: 'what is the name of coffee i did not like']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You are a helpful coffee expert. Use the following CONTEXT, and when useful,\n",
      "use RELEVANT_MEMORY to answer the QUESTION.\n",
      "If the answer is not in the context, say \"I don't know from the provided documents.\"\n",
      "Don't create/invent imaginary sources. Cite source [DOC ID] when you see them.\n",
      "\n",
      "Conversation so far :\n",
      "User: i hate turmeric latte\n",
      "Assistant: I understand you don't like turmeric latte. [doc-2] describes what a turmeric latte (also known as 'golden milk') is – a traditional caffeine-free drink made with milk, turmeric, ginger, cinnamon, and a sweetener, prized for its anti-inflammatory properties.\n",
      "\n",
      "I don't know from the provided documents about alternatives or other information related to disliking turmeric lattes.\n",
      "User: what i did like in coffee\n",
      "Assistant: I don't know from the provided documents about what you liked in coffee.\n",
      "\n",
      "Relevant memory : (facts about the user)\n",
      "None\n",
      "\n",
      "QUESTION:\n",
      "what is the name of coffee i did not like\n",
      "\n",
      "CONTEXT:\n",
      "[doc-1] Ashwagandha coffee is a beverage that blends coffee with Ashwagandha root powder, an adaptogen used in Ayurvedic medicine. It's claimed to help reduce stress and anxiety.\n",
      "[doc-4] Mushroom coffee, such as Chaga or Lion's Mane, is a blend that offers lower caffeine levels than regular coffee. It's often promoted for its cognitive and immune-boosting benefits.\n",
      "[doc-3] For a standard espresso shot, the typical brew ratio is 1:2, meaning 18 grams of ground coffee yields a 36-gram liquid shot in about 25-30 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  [Generating response...]\n",
      "\n",
      "Assistant: Based on our conversation, you mentioned you did not like turmeric latte. However, as previously described, a turmeric latte is a traditional caffeine-free drink, not a coffee [doc-2]. I don't know from the provided documents about any coffee that you did not like.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- Formatting helpers --- [cite: 3605]\n",
    "def format_history(history: List[Dict]) -> str:\n",
    "    if not history:\n",
    "        return \"None\"\n",
    "    # Format for the prompt, clearly labeling roles [cite: 3524-3527]\n",
    "    return \"\\n\".join(f\"{h['role'].capitalize()}: {h['content']}\" for h in history)\n",
    "\n",
    "def format_memory(mem_hits: List[Dict]) -> str:\n",
    "    if not mem_hits:\n",
    "        return \"None\"\n",
    "    return \"\\n\".join(f\"- {h['metadata'].get('fact', '')}\" for h in mem_hits)\n",
    "\n",
    "def format_context(doc_hits: List[Dict]) -> str:\n",
    "    if not doc_hits:\n",
    "        return \"None\"\n",
    "    lines = []\n",
    "    for h in doc_hits:\n",
    "        doc_id = h.get(\"id\", \"\")\n",
    "        text = h['metadata'].get(\"text\", \"No text found\")\n",
    "        lines.append(f\"[{doc_id}] {text}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_prompt(query: str, history: List[Dict], doc_hits: List[Dict], mem_hits: List[Dict]) -> str:\n",
    "    return MEMORY_RAG_TEMPLATE.format(\n",
    "        history=format_history(history),\n",
    "        memory_block=format_memory(mem_hits),\n",
    "        query=query,\n",
    "        context_block=format_context(doc_hits)\n",
    "    )\n",
    "\n",
    "# --- 4. THE CONVERSATIONAL TURN ORCHESTRATOR --- [cite: 3614]\n",
    "def memory_rag_turn(user_text: str) -> str:\n",
    "    \"\"\"Performs a complete, stateful RAG turn.\"\"\"\n",
    "\n",
    "    # 1. Update long-term memory with new facts from the user's message\n",
    "    new_facts = extract_user_facts(user_text)\n",
    "    add_memory_facts(new_facts)\n",
    "\n",
    "    # 2. Retrieve from both documents AND long-term memory\n",
    "    print(f\"  [Retrieving docs for: '{user_text}']\")\n",
    "    doc_hits = retrieve_docs(user_text, k=3)\n",
    "    print(f\"  [Retrieving memories for: '{user_text}']\")\n",
    "    mem_hits = retrieve_memory(user_text, k=2)\n",
    "\n",
    "    # 3. Assemble the full prompt\n",
    "    prompt = build_prompt(user_text, chat_history, doc_hits, mem_hits)\n",
    "\n",
    "    print(\"\\n\"*10)\n",
    "    print(prompt)\n",
    "    print(\"\\n\"*10)\n",
    "\n",
    "    # 4. Call the LLM\n",
    "    print(\"  [Generating response...]\")\n",
    "    response = llm.generate_content(prompt)\n",
    "    answer = getattr(response, \"text\", \"Sorry, I couldn't generate a response.\").strip()\n",
    "\n",
    "    # 5. Update short-term memory (the conversation history)\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_text})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    # 6. Show the user the answer\n",
    "    print(f\"\\nAssistant: {answer}\")\n",
    "    return answer\n",
    "\n",
    "# --- 5. INTERACTIVE CHAT LOOP ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Coffee Memory-RAG Bot ---\")\n",
    "    print(\"Type 'exit' to quit. Try stating a preference, like 'I prefer low-caffeine drinks.'\")\n",
    "    while True:\n",
    "        q = input(\"\\nYou: \").strip()\n",
    "        if q.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        if not q:\n",
    "            continue\n",
    "        memory_rag_turn(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1dbf6-f25f-457f-95b8-2ba28b927efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682f93b-7291-4a97-9b46-2ef5251788fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
