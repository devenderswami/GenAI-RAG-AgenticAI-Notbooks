{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24ca404f",
        "outputId": "9477f77e-9f90-44b9-d5a8-a37c85a72dee"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "yCg_l16b1k2-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample \"documents\"\n",
        "documents = [\n",
        "\"Cats are playful pets.\",\n",
        "\"Dogs are loyal animals.\",\n",
        "\"Electric cars are the future of transportation.\"\n",
        "]"
      ],
      "metadata": {
        "id": "Z0ZPH8tg1oDr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs = [doc.lower().replace('.','').split() for doc in documents]"
      ],
      "metadata": {
        "id": "JJ8aYscu1trO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylL4B0_J2UDV",
        "outputId": "61586bd2-67cb-4d08-daca-987e783a7071"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['cats', 'are', 'playful', 'pets'], ['dogs', 'are', 'loyal', 'animals'], ['electric', 'cars', 'are', 'the', 'future', 'of', 'transportation']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a wrod2vec model on documents\n",
        "model = Word2Vec(sentences=tokenized_docs,vector_size=50,window=2,min_count=1,sg=1,workers=1)"
      ],
      "metadata": {
        "id": "joNA2hRt2aEp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the average embedding for a document\n",
        "def get_avg_embedding(model,doc_tokens):\n",
        "  embeddings = [model.wv[word] for word in doc_tokens if word in model.wv]\n",
        "  return np.mean(embeddings,axis=0) if embeddings else np.zeros(model.vector_size)"
      ],
      "metadata": {
        "id": "kF01NTPF31ZF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings for each document\n",
        "doc_embeddings = np.array([get_avg_embedding(model,doc) for doc in tokenized_docs])"
      ],
      "metadata": {
        "id": "LjbY6fh1344H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use the query \"cat\" and get its embedding\n",
        "query_tokens = [\"cat\"]\n",
        "query_embedding = get_avg_embedding(model,query_tokens)"
      ],
      "metadata": {
        "id": "Gk8jHKpC7-pZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity between query and each document\n",
        "# Expected 2D array, got 1D array instead\n",
        "sims = cosine_similarity([query_embedding],doc_embeddings)[0]"
      ],
      "metadata": {
        "id": "GkM78wDH8ey7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the most relevant\n",
        "most_similar_idx = np.argmax(sims)\n",
        "print(\"Query:\", \"cat\")\n",
        "print(\"Most relevant document:\", documents[most_similar_idx])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMU08lgP-aPV",
        "outputId": "6ac8ddb0-dccd-4117-cda0-46d9e4f56248"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: cat\n",
            "Most relevant document: Cats are playful pets.\n"
          ]
        }
      ]
    }
  ]
}