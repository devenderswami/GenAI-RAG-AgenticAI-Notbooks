{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6119be2-5587-43a3-a4aa-9f6aae9c0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade llama-index        # core\n",
    "%pip install --upgrade llama-index-llms-google-genai  # Google / Gemini LLM integration\n",
    "%pip install --upgrade llama-index-embeddings-google-genai  # embeddings via Google GenAI\n",
    "%pip install --upgrade google-generativeai  # underlying Google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651c6d4-05a7-410d-96e6-f6caad8a2b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.304\n",
    "!pip install langchain-google-genai==0.0.5\n",
    "!pip install google-generativeai==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec2a4f-1b70-4e92-a5b5-081da80f57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== IMPORTS (all in one place)==========================\n",
    "import os, json, textwrap\n",
    "from typing import Tuple, Dict\n",
    "# ---- LlamaIndex (for RAG over Pinecone) ----\n",
    "from pinecone import Pinecone\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290a6a3-ef04-426e-a143-ed37ca2edd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "print(\"ðŸŽ‰ ALL IMPORTS WORKING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fdddf-59dd-4f59-827c-d9b24fd928f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 1) EMBEDDING / LLM LAYER\n",
    "# (match Pinecone: you embedded with text-embedding-004 â†’ 768-d)\n",
    "#================================================================================\n",
    "Settings.embed_model = GoogleGenAIEmbedding(\n",
    " model_name=\"models/text-embedding-004\",\n",
    " api_key=\"\"\n",
    ")\n",
    "# LlamaIndex LLM (used only for grounded synthesis in the tool)\n",
    "Settings.llm = GoogleGenAI(\n",
    " model=\"gemini-2.5-flash\",\n",
    " api_key=\"\",\n",
    " temperature=0.2,\n",
    " max_tokens=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af43b6c-50f8-46f9-b329-5269ffb54685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain chat LLM for the AGENT (same model for consistency)\n",
    "agent_llm = ChatGoogleGenerativeAI(\n",
    " model=\"gemini-2.5-flash\",\n",
    " api_key=\"\",\n",
    " temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b252d-7983-4e3b-825b-6a65490a0ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 2) DATA / INDEX LAYER (Pinecone ONLY; no local PDFs)\n",
    "INDEX_NAME = \"coffeeindex\"\n",
    "TEXT_KEY = \"text\" # change if your field is e.g. \"page_content\"\n",
    "pc = Pinecone(api_key=\"\")\n",
    "pc_index = pc.Index(INDEX_NAME)\n",
    "vstore = PineconeVectorStore(pinecone_index=pc_index,text_key=TEXT_KEY)\n",
    "li_index = VectorStoreIndex.from_vector_store(vstore) # uses Settings.embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c3e5d-5bd8-4a0c-940c-c8ce83b471c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 3) QUERY LAYER -- LlamaIndex â†’ wrapped as a LangChain Tool(\"ask_docs\")\n",
    "# - Retrieval first (no LLM)\n",
    "# - Single-shot, grounded prompt: \"Use ONLY the CONTEXT\"\n",
    "# - Inline citations like [S1]; we also return a machine-readablesources map\n",
    "# - Guardrail: if unsupported â†’ return the fallback string (agent can failover)\n",
    "#================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a9c2a-9579-4fdc-b024-2dfe2bd32c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FALLBACK_STR = \"No support in retrieved context.\"\n",
    "\n",
    "def _li_grounded_answer(question: str, k: int = 5, per_source_chars: int = 900) -> Tuple[str, Dict]:\n",
    "    \"\"\"Run LlamaIndex RAG over Pinecone and produce (answer_text, sources_map).\"\"\"\n",
    "\n",
    "    # 1) Retrieve (cost: ONE embedding call with 004)\n",
    "    hits = li_index.as_retriever(similarity_top_k=k).retrieve(question)\n",
    "\n",
    "    # 2) Build CONTEXT with numbered tags + store sources mapping for UI\n",
    "    parts = []\n",
    "    sources_map = {}\n",
    "\n",
    "    for i, h in enumerate(hits, start=1):\n",
    "        tag = f\"S{i}\"\n",
    "        md = h.node.metadata or {}\n",
    "\n",
    "        heading = (\n",
    "            md.get(\"heading\")\n",
    "            or md.get(\"title\")\n",
    "            or \"Untitled\"\n",
    "        )\n",
    "\n",
    "        src = (\n",
    "            md.get(\"url\")\n",
    "            or md.get(\"source\")\n",
    "            or md.get(\"file_path\")\n",
    "            or md.get(\"doc_id\")\n",
    "            or \"N/A\"\n",
    "        )\n",
    "\n",
    "        # Trim text to avoid huge prompt\n",
    "        snippet = h.node.get_content()[:per_source_chars]\n",
    "\n",
    "        # Build context block\n",
    "        parts.append(f\"[{tag}] {heading}\\n{snippet}\")\n",
    "\n",
    "        # Store for UI\n",
    "        sources_map[tag] = {\"heading\": heading, \"source\": src}\n",
    "\n",
    "    # Join all parts into a single context string\n",
    "    context = \"\\n\\n---\\n\\n\".join(parts) if parts else \"(no results)\"\n",
    "\n",
    "    # 3) Strict, short, procedural prompt (single LLM call)\n",
    "    prompt = f\"\"\"\n",
    "You are a concise specialty-coffee expert.\n",
    "\n",
    "RULES:\n",
    "- Use ONLY the CONTEXT below.\n",
    "- If the answer is not fully supported by the CONTEXT, reply exactly:\n",
    "  \"{FALLBACK_STR}\"\n",
    "- 4â€“6 sentences max.\n",
    "- After each factual claim, add an inline citation like [S1], [S2].\n",
    "- Do NOT use outside knowledge.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT (numbered sources):\n",
    "{context}\n",
    "\n",
    "ANSWER (with inline [S#] citations):\n",
    "\"\"\".strip()\n",
    "\n",
    "    # Final LLM call (ONE Gemini generation)\n",
    "    answer = Settings.llm.complete(prompt).text.strip()\n",
    "\n",
    "    return answer, sources_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e196d-f4d6-42a5-9ccc-c5abd929f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# 4) LANGCHAIN AGENT with MEMORY (Memory RAG)\n",
    "# - The agent chooses when to call the 'ask_docs' tool.\n",
    "# - If tool response == fallback, it asks a follow-up (guardrail).\n",
    "#================================================================================\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are a helpful assistant. Use the 'ask_docs' tool for any question \"\n",
    "                \"that might be answered from the user's coffee knowledge base. \"\n",
    "                f\"If the tool returns '{FALLBACK_STR}', ask a brief follow-up \"\n",
    "                \"question to clarify.\"\n",
    "            ),\n",
    "        ),\n",
    "\n",
    "        # Memory placeholder (conversation history)\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "\n",
    "        # Current user input\n",
    "        (\"human\", \"{input}\"),\n",
    "\n",
    "        # ReAct agent scratchpad for tool calls / reasoning\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cb820-95bd-4552-a08d-15e0bdaebdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== EXAMPLES==================================\n",
    "# 1) A doc-grounded question â†’ agent calls ask_docs, returns grounded answer + citations\n",
    "resp1 = executor.invoke({\"input\": \"What is turmeric coffee? Keep itshort.\"})\n",
    "print(\"\\n=== AGENT #1 ===\\n\", textwrap.fill(resp1[\"output\"], 100))\n",
    "# 2) A likely unsupported question â†’ tool returns fallback â†’ agent asks follow-up\n",
    "resp2 = executor.invoke({\"input\": \"Tell me about Ethiopian teaceremonies from 1800s.\"})\n",
    "print(\"\\n=== AGENT #2 ===\\n\", textwrap.fill(resp2[\"output\"], 100))\n",
    "# 3) Another doc question leveraging conversation memory (Memory RAG flavor)\n",
    "resp3 = executor.invoke({\"input\": \"And ideal brewing temperature guidance?\"})\n",
    "print(\"\\n=== AGENT #3 ===\\n\", textwrap.fill(resp3[\"output\"], 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a10288-6069-4802-b7fa-ea19089b5a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
