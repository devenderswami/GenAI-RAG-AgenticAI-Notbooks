{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27445496-c3fc-4644-8ff2-7bfe6c97f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade llama-index        # core\n",
    "%pip install --upgrade llama-index-llms-google-genai  # Google / Gemini LLM integration\n",
    "%pip install --upgrade llama-index-embeddings-google-genai  # embeddings via Google GenAI\n",
    "%pip install --upgrade google-generativeai  # underlying Google SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490445ea-42ce-4884-b2ae-01f9532fb2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2644b-52db-48cd-b14a-12bcc98497be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "from llama_index.core import (\n",
    " Settings,\n",
    " SimpleDirectoryReader,\n",
    " VectorStoreIndex,\n",
    " get_response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d4e24-c4f9-4bdd-a4f0-c6c80d83c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.core import KeywordTableIndex\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4f4ab-9632-4b6e-9a77-5d4e2fa517f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: instrumentation to count LLM calls (version-safe)\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "try:\n",
    " from llama_index.core.callbacks import TokenCountingHandler\n",
    " counter = TokenCountingHandler()\n",
    " Settings.callback_manager = CallbackManager([counter])\n",
    "except Exception:\n",
    " counter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec3cd0-437e-4e1b-ac8e-ec2b41e82791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ 1) TURN OFF LLM WHILE LOADING OFFLINE DOCS ============================\n",
    "# This prevents any accidental LLM usage during PDF loading / keyword index build.\n",
    "Settings.llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429027f-20cb-4550-b352-6d0111e4f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDFs locally (NO LLM CALLS here)\n",
    "try:\n",
    " docs = SimpleDirectoryReader(\"./GenAI/GenAI-NoteBooks/coffee_pages\",required_exts=[\".html\"]).load_data()\n",
    "except TypeError:\n",
    " docs = SimpleDirectoryReader(\"./GenAI/GenAI-NoteBooks/coffee_pages\",file_exts=[\".html\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da087a4a-ae1a-4f6f-989d-3036d505770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pure keyword lookup index from the PDFs (NO LLM CALLS)\n",
    "kw_idx = KeywordTableIndex.from_documents(docs, transformations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fa591-200a-4fc9-8990-c85610067da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ 2) ENABLE EMBEDDING + LLM FOR QUERIES============================\n",
    "# Must match Pinecone's embedding space (you indexed with 004 /768-d)\n",
    "Settings.embed_model = GoogleGenAIEmbedding(\n",
    " model_name=\"models/text-embedding-004\",\n",
    " api_key=\"\"or\n",
    "os.environ[\"GEMINI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c264d8-3941-4347-8ad7-9de5d1c9c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-shot, no tools/AFC (kwargs ignored if unsupported--safe to include)\n",
    "Settings.llm = GoogleGenAI(\n",
    " model=\"gemini-2.5-flash\",\n",
    " api_key=\"\" or\n",
    "os.environ[\"GEMINI_API_KEY\"],\n",
    "temperature=0.2,\n",
    " max_tokens=512,\n",
    " tools=[],\n",
    " tool_config={\"function_calling_config\": \"NONE\",\n",
    "\"max_remote_calls\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6549ed-9ebf-4d9e-8443-4f748622678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ 3) DATA / INDEX (Pinecone semantic side) ============================\n",
    "pc = Pinecone(api_key=\"pcsk_3v68tN_L3G7scFvZJ8FtqsGh4T3yfHS86sXPfnojrAFUfx5D6XnDvHcWrYKy5T4CcRSZXs\")\n",
    "pc_index = pc.Index(\"coffeeindex\")\n",
    "vstore = PineconeVectorStore(pinecone_index=pc_index,text_key=\"text\") # change if you used \"page_content\"\n",
    "sem_idx = VectorStoreIndex.from_vector_store(vstore) # wraps Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4dca09-192e-4af9-b585-e2320307cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Retrievers (no LLM calls; Pinecone will do one 004 embed per query) =================\n",
    "\n",
    "sem_ret = sem_idx.as_retriever(similarity_top_k=5)\n",
    "kw_ret  = kw_idx.as_retriever(similarity_top_k=5)\n",
    "\n",
    "\n",
    "# ================= Hybrid retrieval with NO query expansion =================\n",
    "# (prevents extra LLM work)\n",
    "\n",
    "hybrid = QueryFusionRetriever(\n",
    "    retrievers=[sem_ret, kw_ret],\n",
    "    similarity_top_k=5,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    num_queries=1,      # NO query expansion â†’ 1 means do NOT generate variations\n",
    "    use_async=False\n",
    ")\n",
    "\n",
    "\n",
    "# ================= Force exactly ONE LLM generation per query =================\n",
    "synth = get_response_synthesizer(\n",
    "    llm=Settings.llm,\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n",
    "# ================= Build Query Engine =================\n",
    "qe = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid,\n",
    "    response_synthesizer=synth\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ================= RUN FUNCTION =================\n",
    "def run_one(q: str):\n",
    "    if counter:\n",
    "        counter.reset_counts()\n",
    "\n",
    "    ans = qe.query(q)\n",
    "    print(ans)\n",
    "\n",
    "    if counter:\n",
    "        print(\"LLM calls for this query:\",\n",
    "              getattr(counter, \"total_llm_calls\", \"N/A\"))   # should be 1\n",
    "\n",
    "\n",
    "# ================= Run queries (each => exactly ONE LLM call) =================\n",
    "run_one(\"What is turmeric coffee?\")\n",
    "run_one(\"Find entries mentioning ashwagandha in titles or headings.\")\n",
    "run_one(\"Explain brewing temperature in 2 bullets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc91a5-9733-4906-9548-7624c7a67c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
