{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71a5c71-c73b-484d-a788-177bc3881360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from faiss-cpu) (2.3.3)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.12.0-cp313-cp313-macosx_14_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca588e2b-c6f7-4c4d-b190-9c5eef718e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (77.0.3)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading torch-2.9.0-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:30\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, Pillow, networkx, joblib, hf-xet, fsspec, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Pillow-12.0.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.3 sentence-transformers-5.1.2 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.0 transformers-4.57.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2854047b-35cc-41d7-972d-aa1ca5c0e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,time,psutil,numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab2bc3ad-f86d-435d-9dd4-8d1fee6445be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"FAISS:\",faiss.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "579f09b3-83e0-455b-a218-76b5c3391c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++Embedding the documents +++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c406af9-6b86-4ad7-bed8-d050ab5b3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9793e740-b93b-41d8-b1b3-696f8b293086",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_DIR = Path(\"/Users/devenderswami/GenAI/GenAI-NoteBooks/html_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fd6b196-c355-4f6d-9adf-8cca9cd09c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ['Credit Card Usage Policy Credit Card Billing and Payment Policy Customers must ensure timely payment of their credit card bills to avoid late payment fees and interest charges. The due date for payment will be mentioned on every monthly statement. Partial payments will attract interest on the remaining balance until full payment is made. Rewards and Cashback Each purchase made using the credit card earns reward points or cashback as per the ongoing offer. Reward points can be redeemed for vouchers, merchandise, or converted into statement credit. Security and Fraud Prevention Never share your OTP or CVV with anyone, including bank officials. Immediately report any unauthorized transaction to the bank’s helpline. Lost cards should be blocked instantly using the mobile app or helpline number. Contact For billing disputes or reward redemption queries, email support@creditbank.com or call 1800-900-999.', 'Personal Loan Policy Policy on Pre-Closing a Personal Loan Customers may pre-close their personal loans without any penalty after April 1, 2025. All outstanding interest up to the date of closure must be paid in full. Prepayment is allowed only after the first 6 EMIs have been paid successfully. Documentation Required Valid ID proof (Aadhaar, PAN, or Passport) Loan account statement Signed pre-closure request form Contact For assistance, reach out to support@loanpolicybank.com or visit your nearest branch.']\n",
      "docid ['credit_card_policy', 'sample_policy']\n",
      "file_names ['credit_card_policy.html', 'sample_policy.html']\n"
     ]
    }
   ],
   "source": [
    "# Read Html Files -> Extract Plain Text\n",
    "file_paths = sorted(HTML_DIR.glob(\"*.html\"))\n",
    "texts = []\n",
    "doc_ids = [] #string IDs (e.g., file stems)\n",
    "file_names = [] #optional : full file names\n",
    "\n",
    "for fp in file_paths:\n",
    "    html = fp.read_text(encoding=\"utf-8\",errors=\"ignore\")\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    text = soup.get_text(\" \",strip=True)\n",
    "    if not text:\n",
    "        continue\n",
    "    texts.append(text)\n",
    "    doc_ids.append(fp.stem)\n",
    "    file_names.append(fp.name)\n",
    "\n",
    "print(\"text\",texts)\n",
    "print(\"docid\",doc_ids)\n",
    "print(\"file_names\",file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e8a955f-b504-426a-bcc8-e8042748b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings (matches your previous MiniLM choice)\n",
    "# texts :A list of strings (your cleaned HTML text).\n",
    "# convert_to_numpy=True : Returns the output as a NumPy array instead of PyTorch tensors.\n",
    "# normalize_embeddings=False:If True, embeddings are scaled to unit length (good for cosine). Here it’s left False because FAISS can handle normalization separately.\n",
    "# .astype(\"float32\") Converts data type from 64-bit floats to 32-bit — required by FAISS for speed and memory efficiency.\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "emb = model.encode(\n",
    "    texts,convert_to_numpy=True,normalize_embeddings=False\n",
    ").astype(\"float32\")\n",
    "emb = np.ascontiguousarray(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84f37792-f4eb-42c4-b8ae-3e44e5a7713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs embedded: 2 | Dim: 384\n"
     ]
    }
   ],
   "source": [
    "#3 Shapes /basic sanity\n",
    "N,d = emb.shape\n",
    "print(\"Docs embedded:\",N,\"| Dim:\",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e9902dc-8999-41e2-b68d-eff35fdd42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 (Optional) Map your string IDs to int64 for FAISS IndexIDMap2\n",
    "#This creates a unique numeric ID (int64) for each document string ID (like file name) — because FAISS needs numeric IDs, not text.\n",
    "ids_int64 = np.array([int(hash(s)) & 0x7FFFFFFF for s in doc_ids], dtype=\"int64\")\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10d588e7-d23e-4f83-bd31-900614c46875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you have:\n",
    "# emb: (N, d) float32 contiguous embeddings from your HTML docs\n",
    "# doc_ids: list of string IDs (file stems)\n",
    "# file_names: list of file names (for readability in results)\n",
    "# ids_int64: deterministic int64 IDs ready for FAISS add_with_ids(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc749071-caf2-4eb4-a18b-e7e527d56511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++Upserting into FAISS-HNSW +++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f4d6601-fb83-412c-abb3-35985b00b61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW total vectors 2\n"
     ]
    }
   ],
   "source": [
    "#use cosine via IP on normalized vectors\n",
    "emb_norm = emb.copy()\n",
    "faiss.normalize_L2(emb_norm) #in-place L2 normalization\n",
    "M = 32 #graph degree (higher -> better recall more memory)\n",
    "#if your faiss build supports metric param , prefer METRIC_INNER_PRODUCT:\n",
    "#hnsw = faiss.IndexHNSWFlat(d,M,faiss.METRIC_INNER_PRODUCT)\n",
    "#otherwise this will also works with normalize vectors\n",
    "hnsw = faiss.IndexHNSWFlat(d,M)\n",
    "#Turning knobs\n",
    "hnsw.hnsw.efConstruction = 200  #build time accuracy/ speed trade-off\n",
    "hnsw.hnsw.efSearch = 64 #query-time recall/speed trade-off\n",
    "\n",
    "#Map your Customer IDs \n",
    "hnsw_idmap = faiss.IndexIDMap2(hnsw)\n",
    "hnsw_idmap.add_with_ids(emb_norm,ids_int64)\n",
    "print(\"HNSW total vectors\",hnsw_idmap.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a2592d9-e65b-4a1e-a4ef-1573ffa4c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++Querying into FAISS-HNSW +++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6675a761-1b7e-4b41-bdd9-6e8dc08151bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. id=566911232 score=1.1461 file=sample_policy.html\n",
      "2. id=452395549 score=1.7896 file=credit_card_policy.html\n",
      "3. id=-1 score=3.4028234663852886e+38 file=None\n",
      "4. id=-1 score=3.4028234663852886e+38 file=None\n",
      "5. id=-1 score=3.4028234663852886e+38 file=None\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Prepayment is allowed only after how many EMI's\"\n",
    "## encode query in vector\n",
    "q = model.encode([query_text],# wrap in a list for batch shape(1,d)\n",
    "                  convert_to_numpy = True,\n",
    "                  normalize_embeddings=False #keep false we will normalize manually\n",
    "                ).astype(\"float32\")\n",
    "# normalize the vector\n",
    "faiss.normalize_L2(q)\n",
    "#run the search\n",
    "top_k = 5\n",
    "D,I = hnsw_idmap.search(q,top_k) # D: Similarity scores, I: int64 IDs\n",
    "#Map IDs back to filenames and show result\n",
    "id_to_file = {ids_int64[i]: file_names[i] for i in range(len(file_names))}\n",
    "for rank, (pid,score) in enumerate(zip(I[0].tolist(),D[0].tolist()),start=1):\n",
    "    print(f\"{rank}. id={pid} score={round(score,4)} file={id_to_file.get(pid)}\")\n",
    "                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9636bf4-7212-49f9-a36a-ec2de34e2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++ Upserting IVF ++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cab5616-8a85-4720-b199-acb09b08b751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVF total vectors: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 2 points to 1 centroids: please provide at least 39 training points\n"
     ]
    }
   ],
   "source": [
    "# Assume you already have:\n",
    "# emb -> (N, d) float32 embeddings from your HTML docs\n",
    "# ids_int64 -> np.int64 IDs matching docs\n",
    "# d -> embedding dimension\n",
    "# ---------------------------\n",
    "# 1. Normalize (for cosine search)\n",
    "# ---------------------------\n",
    "emb_norm = emb.copy()\n",
    "faiss.normalize_L2(emb_norm)\n",
    "# ---------------------------\n",
    "# 2. Create IVF index\n",
    "# ---------------------------\n",
    "#nlist = min(64,max(8,len(ids_int64)//5)) # number of coarse clusters\n",
    "nlist = min(2, max(1, len(ids_int64)//2))\n",
    "quantizer = faiss.IndexFlatIP(d)# quantizer (flat IP for cosine)\n",
    "# IVF index with Flat storage\n",
    "ivf = faiss.IndexIVFFlat(\n",
    "quantizer,\n",
    "d,\n",
    "nlist,faiss.METRIC_INNER_PRODUCT)\n",
    "# ---------------------------\n",
    "# 3. Train the IVF index\n",
    "# ---------------------------\n",
    "# Training is required before adding vectors\n",
    "ivf.train(emb_norm)\n",
    "# ---------------------------\n",
    "# 4. Wrap with IDMap and add vectors\n",
    "# ---------------------------\n",
    "ivf_idmap = faiss.IndexIDMap(ivf)\n",
    "ivf_idmap.add_with_ids(emb_norm,ids_int64)\n",
    "# ---------------------------\n",
    "# 5. Set query-time parameter\n",
    "# ---------------------------\n",
    "ivf_idmap.nprobe = min(8,nlist) #number of cluster to search at query time\n",
    "print(\"IVF total vectors:\", ivf_idmap.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26547bbb-aaf4-49bb-8282-ed24773357c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++ Querying IVF ++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c730fd9-6824-48c6-a133-b46c91c02f09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. id=566911232 score=0.0521file=sample_policy.html\n",
      "2. id=452395549 score=0.0079file=credit_card_policy.html\n",
      "3. id=-1 score=-3.4028234663852886e+38file=None\n",
      "4. id=-1 score=-3.4028234663852886e+38file=None\n",
      "5. id=-1 score=-3.4028234663852886e+38file=None\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 1 -- Prepare the query text\n",
    "# =============================\n",
    "query_text = \"AI in simulations\"\n",
    "# =============================\n",
    "# 2 -- Encode query into a vector\n",
    "# =============================\n",
    "q = model.encode(\n",
    "[query_text], # batch shape (1, d)\n",
    "convert_to_numpy=True,\n",
    "normalize_embeddings=False # we'll normalize manually\n",
    ").astype(\"float32\")\n",
    "# =============================\n",
    "# 3 -- Normalize the vector\n",
    "# =============================\n",
    "# For cosine similarity with IVF (IP), normalize query too\n",
    "faiss.normalize_L2(q)\n",
    "# =============================\n",
    "# 4 -- (Optional) tune IVF query breadth\n",
    "# =============================\n",
    "# nprobe = how many coarse clusters to search; higher -> better recall,slower\n",
    "# You likely set this when building the index; can tweak here if needed\n",
    "nlist = min(2, max(1, len(ids_int64)//2))\n",
    "#ivf_idmap.nprobe = min(8, ivf_idmap.nlist) # e.g., 8; try 16/32 for higher recall\n",
    "#ivf_idmap.index.nprobe = min(2,nlist)\n",
    "# =============================\n",
    "# 5 -- Run the search\n",
    "# =============================\n",
    "top_k = 5\n",
    "D, I = ivf_idmap.search(q, top_k) # D: similarity scores, I: int64 IDs\n",
    "# =============================\n",
    "# 6 -- Map IDs back to filenames and show results\n",
    "# =============================\n",
    "id_to_file = {ids_int64[i]: file_names[i] for i in range(len(file_names))}\n",
    "for rank, (pid, score) in enumerate(zip(I[0].tolist(), D[0].tolist()),start=1):\n",
    "  print(f\"{rank}. id={pid} score={round(score,4)}file={id_to_file.get(pid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e9be5-afe9-4650-b8ee-57b4ada2847b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
